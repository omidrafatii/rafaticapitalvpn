import requests
import base64
import re
import random
import urllib.parse
from collections import OrderedDict

# ====== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ======
# Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ (Ù‡Ø± URL Ú©Ù‡ Ù…Ø­ØªÙˆØ§Ø´ÙˆÙ† Ù…Ø´Ø§Ø¨Ù‡ subscription Ø¨Ø§Ø´Ù‡)
sources = [
    "https://raw.githubusercontent.com/v2rayCrow/Sub-Link-Output/main/sub.txt",
    # "https://raw.githubusercontent.com/other/source1/main/sub.txt",
    # "https://example.com/another-sub.txt",
]

# Ø§Ø³Ù… Ø³Ø§ÛŒØª Ø¨Ø±Ø§ÛŒ ØªØ¨Ù„ÛŒØº
site_name = "rafaticapital.ir"

# ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø§Ø®Ù„ Ø±ÛŒÙ¾Ùˆ
output_file = "sub.txt"

# Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ø³Ø±ÙˆØ± Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ (None Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù…Ø­Ø¯ÙˆØ¯)
MAX_SERVERS = None  # ÛŒØ§ Ù…Ø«Ù„Ø§Ù‹ 200

# Ù„ÛŒØ³Øª Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ (Ø§ÛŒÙ…ÙˆØ¬ÛŒ) â€” Ù‡Ø± Ú©Ø¯ÙˆÙ… Ø®ÙˆØ§Ø³ØªÛŒ Ø§Ø¶Ø§ÙÙ‡/Ø­Ø°Ù Ú©Ù†
flags = [
    "ğŸ‡®ğŸ‡·","ğŸ‡ºğŸ‡¸","ğŸ‡¬ğŸ‡§","ğŸ‡©ğŸ‡ª","ğŸ‡«ğŸ‡·","ğŸ‡¨ğŸ‡¦","ğŸ‡¦ğŸ‡º","ğŸ‡¯ğŸ‡µ","ğŸ‡°ğŸ‡·","ğŸ‡®ğŸ‡³",
    "ğŸ‡·ğŸ‡º","ğŸ‡¦ğŸ‡ª","ğŸ‡¸ğŸ‡¬","ğŸ‡³ğŸ‡±","ğŸ‡¨ğŸ‡­","ğŸ‡¸ğŸ‡ª","ğŸ‡³ğŸ‡´","ğŸ‡ªğŸ‡¸","ğŸ‡®ğŸ‡¹","ğŸ‡§ğŸ‡·"
]
# =====================

def fetch_source(url):
    try:
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        return r.text.strip()
    except Exception as e:
        print(f"âš ï¸ Failed to fetch {url}: {e}")
        return ""

def decode_maybe_base64(data):
    # Ø§Ú¯Ø± Ú©Ù„ Ø¯Ø§Ø¯Ù‡ base64 Ø§Ù†Ú©ÙØ¯ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù‡ Ø¯ÛŒÚ©Ø¯ Ú©Ù†ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù† Ù‡Ù…ÙˆÙ† Ù…ØªÙ†
    data = data.strip()
    if not data:
        return ""
    # heuristic: Ø§Ú¯Ø± ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ø­Ø±ÙˆÙ base64 Ùˆ Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯ØŒ ØªÙ„Ø§Ø´ Ú©Ù†
    try:
        decoded = base64.b64decode(data).decode("utf-8")
        # Ø§Ú¯Ø± Ø¨Ø¹Ø¯ Ø§Ø² Ø¯ÛŒÚ©Ø¯ Ø´Ø¯Ù† Ù„Ø§ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø´ØªØŒ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†
        if "\n" in decoded or "://" in decoded:
            return decoded
    except Exception:
        pass
    return data

def normalize_and_split(data):
    lines = []
    for line in data.splitlines():
        s = line.strip()
        if s:
            lines.append(s)
    return lines

def unique_preserve_order(seq):
    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
    return list(OrderedDict.fromkeys(seq))

def add_remarks_and_flags(links):
    new_links = []
    for i, link in enumerate(links, start=1):
        # Ø­Ø°Ù Ù‡Ø± remark Ù‚Ø¯ÛŒÙ…ÛŒ (Ø¨Ø¹Ø¯ Ø§Ø² #) ØªØ§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒÙ…
        link = re.sub(r'#.*', '', link)

        # ÛŒÚ© Ù¾Ø±Ú†Ù… ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø¯Ø§Ø±
        flag = random.choice(flags)

        # Ù…ØªÙ† remark Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ù†Ù…Ø§ÛŒØ´ Ø¨Ø¯ÛŒÙ…
        remark_text = f"Rafati Capital {i} - {site_name} {flag}"

        # percent-encode Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ…Ù† Ø¨ÙˆØ¯Ù†
        encoded_remark = urllib.parse.quote(remark_text, safe='')

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† remark Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù„ÛŒÙ†Ú© Ø¨Ø§ #
        link_with_remark = f"{link}#{encoded_remark}"

        new_links.append(link_with_remark)
    return new_links

def main():
    all_links = []
    for src in sources:
        raw = fetch_source(src)
        if not raw:
            continue
        decoded = decode_maybe_base64(raw)
        lines = normalize_and_split(decoded)
        all_links.extend(lines)

    # ÛŒÚ©ØªØ§ Ú©Ø±Ø¯Ù† Ùˆ Ø­ÙØ¸ ØªØ±ØªÛŒØ¨
    all_links = unique_preserve_order(all_links)

    # Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø§ÛŒÙ†Ø¬Ø§ shuffle Ú©Ù†ÛŒ Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒ ØªØ±ØªÛŒØ¨ Ø±Ù†Ø¯ÙˆÙ… Ø¨Ø§Ø´Ù‡
    random.shuffle(all_links)

    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø§Ú¯Ø± Ø®ÙˆØ§Ø³ØªÛŒ
    if MAX_SERVERS:
        all_links = all_links[:MAX_SERVERS]

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† remarks Ùˆ flags
    new_links = add_remarks_and_flags(all_links)

    # Ø§Ù†Ú©Ø¯ Ù…Ø¬Ø¯Ø¯ Ø¨Ù‡ base64 Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
    encoded = base64.b64encode("\n".join(new_links).encode()).decode()

    with open(output_file, "w") as f:
        f.write(encoded)

    print(f"âœ… Updated {len(new_links)} servers into {output_file}")

if __name__ == "__main__":
    main()
